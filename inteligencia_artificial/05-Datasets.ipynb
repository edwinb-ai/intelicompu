{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "El origen y centro de las técnicas de _Machine Learning_ e _Inteligencia Artificial_ con TensorFlow son los **datos**, y por esta razón se deben tomar ciertos conjuntos de datos clásicos para diversas ramas de aplicación. En esta sección se mostrará cómo se importan los conjuntos de datos que se utilizarán a lo largo del curso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset de las flores de género _Iris_\n",
    "\n",
    "El _Iris_ dataset, atribuido a [R. Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher), padre de la estadística moderna, es uno de los datasets clásicos del reconocimiento de patrones. La descripción corta de este conjunto de datos es como sigue.\n",
    "\n",
    "Se tienen **_tres_ especies** diferentes de flores de género _Iris_:\n",
    "1. _Iris setosa_\n",
    "2. _Iris virginica_\n",
    "3. _Iris versicolor_\n",
    "\n",
    "Por cada una de estas especies se tienen **_cuatro_ características** principales:\n",
    "1. Longitud de pétalo\n",
    "2. Ancho de pétalo\n",
    "3. Longitud de sépalo\n",
    "4. Ancho de sépalo\n",
    "\n",
    "Lo interesante de este dataset es que dos de las especies son _no_ separables linealmente, mientras que la especie restante _si_ es separable linealmente. Esto se refiere a que se puede dibujar una línea recta entre los datos de las especies y se podrá ver una separación exacta, mientras que para el caso en que _no_ lo son no existe una línea recta que las pueda separar. Para más información sobre la separabilidad lineal, ir [aquí.](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)\n",
    "\n",
    "La [página](https://en.wikipedia.org/wiki/Iris_flower_data_set) de Wikipedia del dataset contiene información muy útil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn permite importar este dataset relativamente fácil\n",
    "from sklearn import datasets as skdts\n",
    "iris = skdts.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar el tamaño del conjunto de datos\n",
    "# Aquí el atributo data muestra el conjunto total de características; 50 por cada especie\n",
    "len(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1, 3.5, 1.4, 0.2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La primera entrada contiene 4 elementos\n",
    "iris.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El atributo target nos muestra a qué especie pertenece cada elemento de iris.data\n",
    "len(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Esta es la primera especie, pero para mejor manejo de los algoritmos\n",
    "# se utilizan números\n",
    "iris.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aunque se pueden identificar fácilmente de la siguiente forma\n",
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma podemos identificar lo siguiente:\n",
    "- 0 es _Iris setosa_\n",
    "- 1 es _Iris versicolor_\n",
    "- 2 es _Iris virginica_\n",
    "\n",
    "Hay muy buena información acerca de este dataset en la [página](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris) de _scikit-learn._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nacimientos y pesos\n",
    "\n",
    "Este dataset describe el peso de humanos recién nacidos, así como el de sus respectivas madres, con datos adicionales como:\n",
    "- Raza: puede ser blanco, negro u otro.\n",
    "- Edad de la madre\n",
    "- Hipertensión: si la madre tiene hipertensión.\n",
    "- Fumadora: si la madre fumó durante el embarazo.\n",
    "- Visitas al médico durante embarazo\n",
    "\n",
    "Para más información visitar la [fuente original](https://www.wiley.com/en-us/Applied+Logistic+Regression%2C+3rd+Edition-p-9780470582473). Este dataset viene integrado en paquetes del lenguaje de programación R, y [aquí](https://www.rdocumentation.org/packages/R330/versions/1.0/topics/births.df) se puede encontrar información detallada de cada variable con algunos ejemplos.\n",
    "\n",
    "Este dataset puede ser útil para predicción o clasificación, y es un buen dataset para estadística aplicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importa la librería para hacer solicitudes desde un URL\n",
    "import requests\n",
    "\n",
    "# Se tomarán el dataset desde un repositorio de GitHub\n",
    "birth_file = requests.get('https://raw.githubusercontent.com/joanby/tensorflow/master/datasets/birthweight.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LOW\\tAGE\\tLWT\\tRACE\\tSMOKE\\tPTL\\tHT\\tUI\\tBWT', '1\\t28\\t113\\t1\\t1\\t1\\t0\\t1\\t709', '1\\t29\\t130\\t0\\t0\\t0\\t0\\t1\\t1021', '1\\t34\\t187\\t1\\t1\\t0\\t1\\t0\\t1135', '1\\t25\\t105\\t1\\t0\\t1\\t1\\t0\\t1330', '1\\t25\\t85\\t1\\t0\\t0\\t0\\t1\\t1474', '1\\t27\\t150\\t1\\t0\\t0\\t0\\t0\\t1588', '1\\t23\\t97\\t1\\t0\\t0\\t0\\t1\\t1588', '1\\t24\\t128\\t1\\t0\\t1\\t0\\t0\\t1701', '1\\t24\\t132\\t1\\t0\\t0\\t1\\t0\\t1729', '1\\t21\\t165\\t0\\t1\\t0\\t1\\t0\\t1790', '1\\t32\\t105\\t1\\t1\\t0\\t0\\t0\\t1818', '1\\t19\\t91\\t0\\t1\\t1\\t0\\t1\\t1885', '1\\t25\\t115\\t1\\t0\\t0\\t0\\t0\\t1893', '1\\t16\\t130\\t1\\t0\\t0\\t0\\t0\\t1899', '1\\t25\\t92\\t0\\t1\\t0\\t0\\t0\\t1928', '1\\t20\\t150\\t0\\t1\\t0\\t0\\t0\\t1928', '1\\t21\\t190\\t1\\t0\\t0\\t0\\t1\\t1928', '1\\t24\\t155\\t0\\t1\\t1\\t0\\t0\\t1936', '1\\t21\\t103\\t1\\t0\\t0\\t0\\t0\\t1970', '1\\t20\\t125\\t1\\t0\\t0\\t0\\t1\\t2055', '1\\t25\\t89\\t1\\t0\\t1\\t0\\t0\\t2055', '1\\t19\\t102\\t0\\t0\\t0\\t0\\t0\\t2082', '1\\t19\\t112\\t0\\t1\\t0\\t0\\t1\\t2084', '1\\t26\\t117\\t0\\t1\\t1\\t0\\t1\\t2084', '1\\t24\\t138\\t0\\t0\\t0\\t0\\t0\\t2100', '1\\t17\\t130\\t1\\t1\\t1\\t0\\t1\\t2125', '1\\t20\\t120\\t1\\t1\\t0\\t0\\t0\\t2126', '1\\t22\\t130\\t0\\t1\\t1\\t0\\t1\\t2187', '1\\t27\\t130\\t1\\t0\\t0\\t0\\t1\\t2187', '1\\t20\\t80\\t1\\t1\\t0\\t0\\t1\\t2211', '1\\t17\\t110\\t0\\t1\\t0\\t0\\t0\\t2225', '1\\t25\\t105\\t1\\t0\\t1\\t0\\t0\\t2240', '1\\t20\\t109\\t1\\t0\\t0\\t0\\t0\\t2240', '1\\t18\\t148\\t1\\t0\\t0\\t0\\t0\\t2282', '1\\t18\\t110\\t1\\t1\\t1\\t0\\t0\\t2296', '1\\t20\\t121\\t0\\t1\\t1\\t0\\t1\\t2296', '1\\t21\\t100\\t1\\t0\\t1\\t0\\t0\\t2301', '1\\t26\\t96\\t1\\t0\\t0\\t0\\t0\\t2325', '1\\t31\\t102\\t0\\t1\\t1\\t0\\t0\\t2353', '1\\t15\\t110\\t0\\t0\\t0\\t0\\t0\\t2353', '1\\t23\\t187\\t1\\t1\\t0\\t0\\t0\\t2367', '1\\t20\\t122\\t1\\t1\\t1\\t0\\t0\\t2381', '1\\t24\\t105\\t1\\t1\\t0\\t0\\t0\\t2381', '1\\t15\\t115\\t1\\t0\\t0\\t0\\t1\\t2381', '1\\t23\\t120\\t1\\t0\\t0\\t0\\t0\\t2395', '1\\t30\\t142\\t0\\t1\\t1\\t0\\t0\\t2410', '1\\t22\\t130\\t0\\t1\\t0\\t0\\t0\\t2410', '1\\t17\\t120\\t0\\t1\\t0\\t0\\t0\\t2414', '1\\t23\\t110\\t0\\t1\\t1\\t0\\t0\\t2424', '1\\t17\\t120\\t1\\t0\\t0\\t0\\t0\\t2438', '1\\t26\\t154\\t1\\t0\\t1\\t1\\t0\\t2442', '1\\t20\\t105\\t1\\t0\\t0\\t0\\t0\\t2450', '1\\t26\\t168\\t0\\t1\\t0\\t0\\t0\\t2466', '1\\t14\\t101\\t1\\t1\\t1\\t0\\t0\\t2466', '1\\t28\\t95\\t0\\t1\\t0\\t0\\t0\\t2466', '1\\t14\\t100\\t1\\t0\\t0\\t0\\t0\\t2495', '1\\t23\\t94\\t1\\t1\\t0\\t0\\t0\\t2495', '1\\t17\\t142\\t1\\t0\\t0\\t1\\t0\\t2495', '1\\t21\\t130\\t0\\t1\\t0\\t1\\t0\\t2495', '0\\t19\\t182\\t1\\t0\\t0\\t0\\t1\\t2523', '0\\t33\\t155\\t1\\t0\\t0\\t0\\t0\\t2551', '0\\t20\\t105\\t0\\t1\\t0\\t0\\t0\\t2557', '0\\t21\\t108\\t0\\t1\\t0\\t0\\t1\\t2594', '0\\t18\\t107\\t0\\t1\\t0\\t0\\t1\\t2600', '0\\t21\\t124\\t1\\t0\\t0\\t0\\t0\\t2622', '0\\t22\\t118\\t0\\t0\\t0\\t0\\t0\\t2637', '0\\t17\\t103\\t1\\t0\\t0\\t0\\t0\\t2637', '0\\t29\\t123\\t0\\t1\\t0\\t0\\t0\\t2663', '0\\t26\\t113\\t0\\t1\\t0\\t0\\t0\\t2665', '0\\t19\\t95\\t1\\t0\\t0\\t0\\t0\\t2722', '0\\t19\\t150\\t1\\t0\\t0\\t0\\t0\\t2733', '0\\t22\\t95\\t1\\t0\\t0\\t1\\t0\\t2750', '0\\t30\\t107\\t1\\t0\\t1\\t0\\t1\\t2750', '0\\t18\\t100\\t0\\t1\\t0\\t0\\t0\\t2769', '0\\t18\\t100\\t1\\t1\\t0\\t0\\t0\\t2769', '0\\t15\\t98\\t1\\t0\\t0\\t0\\t0\\t2778', '0\\t25\\t118\\t0\\t1\\t0\\t0\\t0\\t2782', '0\\t20\\t120\\t1\\t0\\t0\\t0\\t1\\t2807', '0\\t28\\t120\\t0\\t1\\t0\\t0\\t0\\t2821', '0\\t32\\t121\\t1\\t0\\t0\\t0\\t0\\t2835', '0\\t31\\t100\\t0\\t0\\t0\\t0\\t1\\t2835', '0\\t36\\t202\\t0\\t0\\t0\\t0\\t0\\t2836', '0\\t28\\t120\\t1\\t0\\t0\\t0\\t0\\t2863', '0\\t25\\t120\\t1\\t0\\t0\\t0\\t1\\t2877', '0\\t28\\t167\\t0\\t0\\t0\\t0\\t0\\t2877', '0\\t17\\t122\\t0\\t1\\t0\\t0\\t0\\t2906', '0\\t29\\t150\\t0\\t0\\t0\\t0\\t0\\t2920', '0\\t26\\t168\\t1\\t1\\t0\\t0\\t0\\t2920', '0\\t17\\t113\\t1\\t0\\t0\\t0\\t0\\t2920', '0\\t17\\t113\\t1\\t0\\t0\\t0\\t0\\t2920', '0\\t24\\t90\\t0\\t1\\t1\\t0\\t0\\t2948', '0\\t35\\t121\\t1\\t1\\t1\\t1\\t0\\t2948', '0\\t25\\t155\\t0\\t1\\t1\\t0\\t0\\t2977', '0\\t25\\t125\\t1\\t0\\t0\\t0\\t0\\t2977', '0\\t29\\t140\\t0\\t1\\t0\\t0\\t0\\t2977', '0\\t19\\t138\\t0\\t1\\t0\\t1\\t0\\t2977', '0\\t27\\t124\\t0\\t1\\t0\\t0\\t0\\t2992', '0\\t31\\t115\\t0\\t1\\t0\\t0\\t0\\t3005', '0\\t33\\t109\\t0\\t1\\t0\\t0\\t0\\t3033', '0\\t21\\t185\\t1\\t1\\t0\\t0\\t0\\t3042', '0\\t19\\t189\\t0\\t0\\t0\\t0\\t0\\t3062', '0\\t23\\t130\\t1\\t0\\t0\\t0\\t0\\t3062', '0\\t21\\t160\\t0\\t0\\t0\\t0\\t0\\t3062', '0\\t18\\t90\\t0\\t1\\t0\\t0\\t1\\t3076', '0\\t18\\t90\\t0\\t1\\t0\\t0\\t1\\t3076', '0\\t32\\t132\\t0\\t0\\t0\\t0\\t0\\t3080', '0\\t19\\t132\\t1\\t0\\t0\\t0\\t0\\t3090', '0\\t24\\t115\\t0\\t0\\t0\\t0\\t0\\t3090', '0\\t22\\t85\\t1\\t1\\t0\\t0\\t0\\t3090', '0\\t22\\t120\\t0\\t0\\t0\\t1\\t0\\t3100', '0\\t23\\t128\\t1\\t0\\t0\\t0\\t0\\t3104', '0\\t22\\t130\\t0\\t1\\t0\\t0\\t0\\t3132', '0\\t30\\t95\\t0\\t1\\t0\\t0\\t0\\t3147', '0\\t19\\t115\\t1\\t0\\t0\\t0\\t0\\t3175', '0\\t16\\t110\\t1\\t0\\t0\\t0\\t0\\t3175', '0\\t21\\t110\\t1\\t1\\t0\\t0\\t1\\t3203', '0\\t30\\t153\\t1\\t0\\t0\\t0\\t0\\t3203', '0\\t20\\t103\\t1\\t0\\t0\\t0\\t0\\t3203', '0\\t17\\t119\\t1\\t0\\t0\\t0\\t0\\t3225', '0\\t17\\t119\\t1\\t0\\t0\\t0\\t0\\t3225', '0\\t23\\t119\\t1\\t0\\t0\\t0\\t0\\t3232', '0\\t24\\t110\\t1\\t0\\t0\\t0\\t0\\t3232', '0\\t28\\t140\\t0\\t0\\t0\\t0\\t0\\t3234', '0\\t26\\t133\\t1\\t1\\t0\\t0\\t0\\t3260', '0\\t20\\t169\\t1\\t0\\t1\\t0\\t1\\t3274', '0\\t24\\t115\\t1\\t0\\t0\\t0\\t0\\t3274', '0\\t28\\t250\\t1\\t1\\t0\\t0\\t0\\t3303', '0\\t20\\t141\\t0\\t0\\t0\\t0\\t1\\t3317', '0\\t22\\t158\\t1\\t0\\t1\\t0\\t0\\t3317', '0\\t22\\t112\\t0\\t1\\t1\\t0\\t0\\t3317', '0\\t31\\t150\\t1\\t1\\t0\\t0\\t0\\t3321', '0\\t23\\t115\\t1\\t1\\t0\\t0\\t0\\t3331', '0\\t16\\t112\\t1\\t0\\t0\\t0\\t0\\t3374', '0\\t16\\t135\\t0\\t1\\t0\\t0\\t0\\t3374', '0\\t18\\t229\\t1\\t0\\t0\\t0\\t0\\t3402', '0\\t25\\t140\\t0\\t0\\t0\\t0\\t0\\t3416', '0\\t32\\t134\\t0\\t1\\t1\\t0\\t0\\t3430', '0\\t20\\t121\\t1\\t1\\t0\\t0\\t0\\t3444', '0\\t23\\t190\\t0\\t0\\t0\\t0\\t0\\t3459', '0\\t22\\t131\\t0\\t0\\t0\\t0\\t0\\t3460', '0\\t32\\t170\\t0\\t0\\t0\\t0\\t0\\t3473', '0\\t30\\t110\\t1\\t0\\t0\\t0\\t0\\t3475', '0\\t20\\t127\\t1\\t0\\t0\\t0\\t0\\t3487', '0\\t23\\t123\\t1\\t0\\t0\\t0\\t0\\t3544', '0\\t17\\t120\\t1\\t1\\t0\\t0\\t0\\t3572', '0\\t19\\t105\\t1\\t0\\t0\\t0\\t0\\t3572', '0\\t23\\t130\\t0\\t0\\t0\\t0\\t0\\t3586', '0\\t36\\t175\\t0\\t0\\t0\\t0\\t0\\t3600', '0\\t22\\t125\\t0\\t0\\t0\\t0\\t0\\t3614', '0\\t24\\t133\\t0\\t0\\t0\\t0\\t0\\t3614', '0\\t21\\t134\\t1\\t0\\t0\\t0\\t0\\t3629', '0\\t19\\t235\\t0\\t1\\t0\\t1\\t0\\t3629', '0\\t25\\t200\\t0\\t0\\t1\\t0\\t1\\t3637', '0\\t16\\t135\\t0\\t1\\t0\\t0\\t0\\t3643', '0\\t29\\t135\\t0\\t0\\t0\\t0\\t0\\t3651', '0\\t29\\t154\\t0\\t0\\t0\\t0\\t0\\t3651', '0\\t19\\t147\\t0\\t1\\t0\\t0\\t0\\t3651', '0\\t19\\t147\\t0\\t1\\t0\\t0\\t0\\t3651', '0\\t30\\t137\\t0\\t0\\t0\\t0\\t0\\t3699', '0\\t24\\t110\\t0\\t0\\t0\\t0\\t0\\t3728', '0\\t19\\t184\\t0\\t1\\t0\\t1\\t0\\t3756', '0\\t24\\t110\\t0\\t0\\t1\\t0\\t0\\t3770', '0\\t23\\t110\\t0\\t0\\t0\\t0\\t0\\t3770', '0\\t20\\t120\\t1\\t0\\t0\\t0\\t0\\t3770', '0\\t25\\t141\\t0\\t0\\t0\\t1\\t0\\t3790', '0\\t30\\t112\\t0\\t0\\t0\\t0\\t0\\t3799', '0\\t22\\t169\\t0\\t0\\t0\\t0\\t0\\t3827', '0\\t18\\t120\\t0\\t1\\t0\\t0\\t0\\t3856', '0\\t16\\t170\\t1\\t0\\t0\\t0\\t0\\t3860', '0\\t32\\t186\\t0\\t0\\t0\\t0\\t0\\t3860', '0\\t18\\t120\\t1\\t0\\t0\\t0\\t0\\t3884', '0\\t29\\t130\\t0\\t1\\t0\\t0\\t0\\t3884', '0\\t33\\t117\\t0\\t0\\t0\\t0\\t1\\t3912', '0\\t20\\t170\\t0\\t1\\t0\\t0\\t0\\t3940', '0\\t28\\t134\\t1\\t0\\t0\\t0\\t0\\t3941', '0\\t14\\t135\\t0\\t0\\t1\\t0\\t0\\t3941', '0\\t28\\t130\\t1\\t0\\t0\\t0\\t0\\t3969', '0\\t25\\t120\\t0\\t0\\t0\\t0\\t0\\t3983', '0\\t16\\t135\\t1\\t0\\t0\\t0\\t0\\t3997', '0\\t20\\t158\\t0\\t0\\t0\\t0\\t0\\t3997', '0\\t26\\t160\\t0\\t0\\t0\\t0\\t0\\t4054', '0\\t21\\t115\\t0\\t0\\t0\\t0\\t0\\t4054', '0\\t22\\t129\\t0\\t0\\t0\\t0\\t0\\t4111', '0\\t25\\t130\\t0\\t0\\t0\\t0\\t0\\t4153', '0\\t31\\t120\\t0\\t0\\t0\\t0\\t0\\t4167', '0\\t35\\t170\\t0\\t0\\t1\\t0\\t0\\t4174', '0\\t19\\t120\\t0\\t1\\t0\\t1\\t0\\t4238', '0\\t24\\t216\\t0\\t0\\t0\\t0\\t0\\t4593', '0\\t45\\t123\\t0\\t0\\t1\\t0\\t0\\t4990', '']\n"
     ]
    }
   ],
   "source": [
    "# Importar todos los datos como texto, y separarlo por cada linea\n",
    "birth_data = birth_file.text.splitlines()\n",
    "print(birth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['LOW', 'AGE', 'LWT', 'RACE', 'SMOKE', 'PTL', 'HT', 'UI', 'BWT'], ['1', '28', '113', '1', '1', '1', '0', '1', '709'], ['1', '29', '130', '0', '0', '0', '0', '1', '1021'], ['1', '34', '187', '1', '1', '0', '1', '0', '1135'], ['1', '25', '105', '1', '0', '1', '1', '0', '1330'], ['1', '25', '85', '1', '0', '0', '0', '1', '1474'], ['1', '27', '150', '1', '0', '0', '0', '0', '1588'], ['1', '23', '97', '1', '0', '0', '0', '1', '1588'], ['1', '24', '128', '1', '0', '1', '0', '0', '1701'], ['1', '24', '132', '1', '0', '0', '1', '0', '1729'], ['1', '21', '165', '0', '1', '0', '1', '0', '1790'], ['1', '32', '105', '1', '1', '0', '0', '0', '1818'], ['1', '19', '91', '0', '1', '1', '0', '1', '1885'], ['1', '25', '115', '1', '0', '0', '0', '0', '1893'], ['1', '16', '130', '1', '0', '0', '0', '0', '1899'], ['1', '25', '92', '0', '1', '0', '0', '0', '1928'], ['1', '20', '150', '0', '1', '0', '0', '0', '1928'], ['1', '21', '190', '1', '0', '0', '0', '1', '1928'], ['1', '24', '155', '0', '1', '1', '0', '0', '1936'], ['1', '21', '103', '1', '0', '0', '0', '0', '1970'], ['1', '20', '125', '1', '0', '0', '0', '1', '2055'], ['1', '25', '89', '1', '0', '1', '0', '0', '2055'], ['1', '19', '102', '0', '0', '0', '0', '0', '2082'], ['1', '19', '112', '0', '1', '0', '0', '1', '2084'], ['1', '26', '117', '0', '1', '1', '0', '1', '2084'], ['1', '24', '138', '0', '0', '0', '0', '0', '2100'], ['1', '17', '130', '1', '1', '1', '0', '1', '2125'], ['1', '20', '120', '1', '1', '0', '0', '0', '2126'], ['1', '22', '130', '0', '1', '1', '0', '1', '2187'], ['1', '27', '130', '1', '0', '0', '0', '1', '2187'], ['1', '20', '80', '1', '1', '0', '0', '1', '2211'], ['1', '17', '110', '0', '1', '0', '0', '0', '2225'], ['1', '25', '105', '1', '0', '1', '0', '0', '2240'], ['1', '20', '109', '1', '0', '0', '0', '0', '2240'], ['1', '18', '148', '1', '0', '0', '0', '0', '2282'], ['1', '18', '110', '1', '1', '1', '0', '0', '2296'], ['1', '20', '121', '0', '1', '1', '0', '1', '2296'], ['1', '21', '100', '1', '0', '1', '0', '0', '2301'], ['1', '26', '96', '1', '0', '0', '0', '0', '2325'], ['1', '31', '102', '0', '1', '1', '0', '0', '2353'], ['1', '15', '110', '0', '0', '0', '0', '0', '2353'], ['1', '23', '187', '1', '1', '0', '0', '0', '2367'], ['1', '20', '122', '1', '1', '1', '0', '0', '2381'], ['1', '24', '105', '1', '1', '0', '0', '0', '2381'], ['1', '15', '115', '1', '0', '0', '0', '1', '2381'], ['1', '23', '120', '1', '0', '0', '0', '0', '2395'], ['1', '30', '142', '0', '1', '1', '0', '0', '2410'], ['1', '22', '130', '0', '1', '0', '0', '0', '2410'], ['1', '17', '120', '0', '1', '0', '0', '0', '2414'], ['1', '23', '110', '0', '1', '1', '0', '0', '2424'], ['1', '17', '120', '1', '0', '0', '0', '0', '2438'], ['1', '26', '154', '1', '0', '1', '1', '0', '2442'], ['1', '20', '105', '1', '0', '0', '0', '0', '2450'], ['1', '26', '168', '0', '1', '0', '0', '0', '2466'], ['1', '14', '101', '1', '1', '1', '0', '0', '2466'], ['1', '28', '95', '0', '1', '0', '0', '0', '2466'], ['1', '14', '100', '1', '0', '0', '0', '0', '2495'], ['1', '23', '94', '1', '1', '0', '0', '0', '2495'], ['1', '17', '142', '1', '0', '0', '1', '0', '2495'], ['1', '21', '130', '0', '1', '0', '1', '0', '2495'], ['0', '19', '182', '1', '0', '0', '0', '1', '2523'], ['0', '33', '155', '1', '0', '0', '0', '0', '2551'], ['0', '20', '105', '0', '1', '0', '0', '0', '2557'], ['0', '21', '108', '0', '1', '0', '0', '1', '2594'], ['0', '18', '107', '0', '1', '0', '0', '1', '2600'], ['0', '21', '124', '1', '0', '0', '0', '0', '2622'], ['0', '22', '118', '0', '0', '0', '0', '0', '2637'], ['0', '17', '103', '1', '0', '0', '0', '0', '2637'], ['0', '29', '123', '0', '1', '0', '0', '0', '2663'], ['0', '26', '113', '0', '1', '0', '0', '0', '2665'], ['0', '19', '95', '1', '0', '0', '0', '0', '2722'], ['0', '19', '150', '1', '0', '0', '0', '0', '2733'], ['0', '22', '95', '1', '0', '0', '1', '0', '2750'], ['0', '30', '107', '1', '0', '1', '0', '1', '2750'], ['0', '18', '100', '0', '1', '0', '0', '0', '2769'], ['0', '18', '100', '1', '1', '0', '0', '0', '2769'], ['0', '15', '98', '1', '0', '0', '0', '0', '2778'], ['0', '25', '118', '0', '1', '0', '0', '0', '2782'], ['0', '20', '120', '1', '0', '0', '0', '1', '2807'], ['0', '28', '120', '0', '1', '0', '0', '0', '2821'], ['0', '32', '121', '1', '0', '0', '0', '0', '2835'], ['0', '31', '100', '0', '0', '0', '0', '1', '2835'], ['0', '36', '202', '0', '0', '0', '0', '0', '2836'], ['0', '28', '120', '1', '0', '0', '0', '0', '2863'], ['0', '25', '120', '1', '0', '0', '0', '1', '2877'], ['0', '28', '167', '0', '0', '0', '0', '0', '2877'], ['0', '17', '122', '0', '1', '0', '0', '0', '2906'], ['0', '29', '150', '0', '0', '0', '0', '0', '2920'], ['0', '26', '168', '1', '1', '0', '0', '0', '2920'], ['0', '17', '113', '1', '0', '0', '0', '0', '2920'], ['0', '17', '113', '1', '0', '0', '0', '0', '2920'], ['0', '24', '90', '0', '1', '1', '0', '0', '2948'], ['0', '35', '121', '1', '1', '1', '1', '0', '2948'], ['0', '25', '155', '0', '1', '1', '0', '0', '2977'], ['0', '25', '125', '1', '0', '0', '0', '0', '2977'], ['0', '29', '140', '0', '1', '0', '0', '0', '2977'], ['0', '19', '138', '0', '1', '0', '1', '0', '2977'], ['0', '27', '124', '0', '1', '0', '0', '0', '2992'], ['0', '31', '115', '0', '1', '0', '0', '0', '3005'], ['0', '33', '109', '0', '1', '0', '0', '0', '3033'], ['0', '21', '185', '1', '1', '0', '0', '0', '3042'], ['0', '19', '189', '0', '0', '0', '0', '0', '3062'], ['0', '23', '130', '1', '0', '0', '0', '0', '3062'], ['0', '21', '160', '0', '0', '0', '0', '0', '3062'], ['0', '18', '90', '0', '1', '0', '0', '1', '3076'], ['0', '18', '90', '0', '1', '0', '0', '1', '3076'], ['0', '32', '132', '0', '0', '0', '0', '0', '3080'], ['0', '19', '132', '1', '0', '0', '0', '0', '3090'], ['0', '24', '115', '0', '0', '0', '0', '0', '3090'], ['0', '22', '85', '1', '1', '0', '0', '0', '3090'], ['0', '22', '120', '0', '0', '0', '1', '0', '3100'], ['0', '23', '128', '1', '0', '0', '0', '0', '3104'], ['0', '22', '130', '0', '1', '0', '0', '0', '3132'], ['0', '30', '95', '0', '1', '0', '0', '0', '3147'], ['0', '19', '115', '1', '0', '0', '0', '0', '3175'], ['0', '16', '110', '1', '0', '0', '0', '0', '3175'], ['0', '21', '110', '1', '1', '0', '0', '1', '3203'], ['0', '30', '153', '1', '0', '0', '0', '0', '3203'], ['0', '20', '103', '1', '0', '0', '0', '0', '3203'], ['0', '17', '119', '1', '0', '0', '0', '0', '3225'], ['0', '17', '119', '1', '0', '0', '0', '0', '3225'], ['0', '23', '119', '1', '0', '0', '0', '0', '3232'], ['0', '24', '110', '1', '0', '0', '0', '0', '3232'], ['0', '28', '140', '0', '0', '0', '0', '0', '3234'], ['0', '26', '133', '1', '1', '0', '0', '0', '3260'], ['0', '20', '169', '1', '0', '1', '0', '1', '3274'], ['0', '24', '115', '1', '0', '0', '0', '0', '3274'], ['0', '28', '250', '1', '1', '0', '0', '0', '3303'], ['0', '20', '141', '0', '0', '0', '0', '1', '3317'], ['0', '22', '158', '1', '0', '1', '0', '0', '3317'], ['0', '22', '112', '0', '1', '1', '0', '0', '3317'], ['0', '31', '150', '1', '1', '0', '0', '0', '3321'], ['0', '23', '115', '1', '1', '0', '0', '0', '3331'], ['0', '16', '112', '1', '0', '0', '0', '0', '3374'], ['0', '16', '135', '0', '1', '0', '0', '0', '3374'], ['0', '18', '229', '1', '0', '0', '0', '0', '3402'], ['0', '25', '140', '0', '0', '0', '0', '0', '3416'], ['0', '32', '134', '0', '1', '1', '0', '0', '3430'], ['0', '20', '121', '1', '1', '0', '0', '0', '3444'], ['0', '23', '190', '0', '0', '0', '0', '0', '3459'], ['0', '22', '131', '0', '0', '0', '0', '0', '3460'], ['0', '32', '170', '0', '0', '0', '0', '0', '3473'], ['0', '30', '110', '1', '0', '0', '0', '0', '3475'], ['0', '20', '127', '1', '0', '0', '0', '0', '3487'], ['0', '23', '123', '1', '0', '0', '0', '0', '3544'], ['0', '17', '120', '1', '1', '0', '0', '0', '3572'], ['0', '19', '105', '1', '0', '0', '0', '0', '3572'], ['0', '23', '130', '0', '0', '0', '0', '0', '3586'], ['0', '36', '175', '0', '0', '0', '0', '0', '3600'], ['0', '22', '125', '0', '0', '0', '0', '0', '3614'], ['0', '24', '133', '0', '0', '0', '0', '0', '3614'], ['0', '21', '134', '1', '0', '0', '0', '0', '3629'], ['0', '19', '235', '0', '1', '0', '1', '0', '3629'], ['0', '25', '200', '0', '0', '1', '0', '1', '3637'], ['0', '16', '135', '0', '1', '0', '0', '0', '3643'], ['0', '29', '135', '0', '0', '0', '0', '0', '3651'], ['0', '29', '154', '0', '0', '0', '0', '0', '3651'], ['0', '19', '147', '0', '1', '0', '0', '0', '3651'], ['0', '19', '147', '0', '1', '0', '0', '0', '3651'], ['0', '30', '137', '0', '0', '0', '0', '0', '3699'], ['0', '24', '110', '0', '0', '0', '0', '0', '3728'], ['0', '19', '184', '0', '1', '0', '1', '0', '3756'], ['0', '24', '110', '0', '0', '1', '0', '0', '3770'], ['0', '23', '110', '0', '0', '0', '0', '0', '3770'], ['0', '20', '120', '1', '0', '0', '0', '0', '3770'], ['0', '25', '141', '0', '0', '0', '1', '0', '3790'], ['0', '30', '112', '0', '0', '0', '0', '0', '3799'], ['0', '22', '169', '0', '0', '0', '0', '0', '3827'], ['0', '18', '120', '0', '1', '0', '0', '0', '3856'], ['0', '16', '170', '1', '0', '0', '0', '0', '3860'], ['0', '32', '186', '0', '0', '0', '0', '0', '3860'], ['0', '18', '120', '1', '0', '0', '0', '0', '3884'], ['0', '29', '130', '0', '1', '0', '0', '0', '3884'], ['0', '33', '117', '0', '0', '0', '0', '1', '3912'], ['0', '20', '170', '0', '1', '0', '0', '0', '3940'], ['0', '28', '134', '1', '0', '0', '0', '0', '3941'], ['0', '14', '135', '0', '0', '1', '0', '0', '3941'], ['0', '28', '130', '1', '0', '0', '0', '0', '3969'], ['0', '25', '120', '0', '0', '0', '0', '0', '3983'], ['0', '16', '135', '1', '0', '0', '0', '0', '3997'], ['0', '20', '158', '0', '0', '0', '0', '0', '3997'], ['0', '26', '160', '0', '0', '0', '0', '0', '4054'], ['0', '21', '115', '0', '0', '0', '0', '0', '4054'], ['0', '22', '129', '0', '0', '0', '0', '0', '4111'], ['0', '25', '130', '0', '0', '0', '0', '0', '4153'], ['0', '31', '120', '0', '0', '0', '0', '0', '4167'], ['0', '35', '170', '0', '0', '1', '0', '0', '4174'], ['0', '19', '120', '0', '1', '0', '1', '0', '4238'], ['0', '24', '216', '0', '0', '0', '0', '0', '4593'], ['0', '45', '123', '0', '0', '1', '0', '0', '4990'], ['']]\n"
     ]
    }
   ],
   "source": [
    "# Pero ahora debemos convertirlo a una estructura de datos que sea\n",
    "# manejable en Python usando numpy\n",
    "birth_data = [i.split('\\t') for i in birth_data]\n",
    "print(birth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LOW', 'AGE', 'LWT', 'RACE', 'SMOKE', 'PTL', 'HT', 'UI', 'BWT']\n"
     ]
    }
   ],
   "source": [
    "# Primero, se quita el encabezado donde viene el nombre de todas las categorías\n",
    "birth_header = birth_data[0]\n",
    "print(birth_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 2.800e+01 1.130e+02 ... 0.000e+00 1.000e+00 7.090e+02]\n",
      " [1.000e+00 2.900e+01 1.300e+02 ... 0.000e+00 1.000e+00 1.021e+03]\n",
      " [1.000e+00 3.400e+01 1.870e+02 ... 1.000e+00 0.000e+00 1.135e+03]\n",
      " ...\n",
      " [0.000e+00 3.500e+01 1.700e+02 ... 0.000e+00 0.000e+00 4.174e+03]\n",
      " [0.000e+00 1.900e+01 1.200e+02 ... 1.000e+00 0.000e+00 4.238e+03]\n",
      " [0.000e+00 2.400e+01 2.160e+02 ... 0.000e+00 0.000e+00 4.593e+03]]\n"
     ]
    }
   ],
   "source": [
    "# Ahora se toman los demás datos, y con la ayuda de numpy todo se puede\n",
    "# hacer fácilmente\n",
    "import numpy as np\n",
    "\n",
    "# Segundo, crear un arreglo con cada fila como arreglo y convertirlo a flotante\n",
    "# todo dentro de una list comprehension y la facilidad de numpy\n",
    "birth_data = np.array([np.array(i, dtype=np.float32) for i in birth_data[1:-2]])\n",
    "print(birth_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA:** Como se puede ver, en la _list comprehension_ se partió la lista original como\n",
    "```python\n",
    "birth_data[1:-2]\n",
    "```\n",
    "y esto es porque el último elemento de la lista es un caracter vacío; esto se puede ver en la celda donde se parte en filas el documento original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precios de casas en Boston, 1970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Este dataset contiene información interesante sobre los costos de casa en Boston en la década de 1970. Esta es la información general del dataset:\n",
    "\n",
    "1. Title: Boston Housing Data\n",
    "\n",
    "2. Sources:\n",
    "\n",
    "   a. Origin:  This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
    "   \n",
    "   b. Creator:  Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978.\n",
    "   \n",
    "   c. Date: July 7, 1993\n",
    "\n",
    "3. Past Usage:\n",
    "    - Used in Belsley, Kuh & Welsch, 'Regression diagnostics ...', Wiley, \n",
    "       1980.   N.B. Various transformations are used in the table on\n",
    "       pages 244-261.\n",
    "    - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning.\n",
    "       In Proceedings on the Tenth International Conference of Machine \n",
    "       Learning, 236-243, University of Massachusetts, Amherst. Morgan\n",
    "       Kaufmann.\n",
    "\n",
    "4. Relevant Information:\n",
    "\n",
    "    - Concerns housing values in suburbs of Boston.\n",
    "\n",
    "5. Number of Instances: 506\n",
    "\n",
    "6. Number of Attributes: 13 continuous attributes (including \"class\" attribute \"MEDV\"), 1 binary-valued attribute.\n",
    "\n",
    "7. Attribute Information:\n",
    "\n",
    "    1. CRIM      per capita crime rate by town\n",
    "    \n",
    "    2. ZN        proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "    \n",
    "    3. INDUS     proportion of non-retail business acres per town\n",
    "    \n",
    "    4. CHAS      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "    \n",
    "    5. NOX       nitric oxides concentration (parts per 10 million)\n",
    "    \n",
    "    6. RM        average number of rooms per dwelling\n",
    "    \n",
    "    7. AGE       proportion of owner-occupied units built prior to 1940\n",
    "    \n",
    "    8. DIS       weighted distances to five Boston employment centres\n",
    "    \n",
    "    9. RAD       index of accessibility to radial highways\n",
    "    \n",
    "    10. TAX      full-value property-tax rate per \\$10,000\n",
    "    \n",
    "    11. PTRATIO  pupil-teacher ratio by town\n",
    "    \n",
    "    12. B        $1000(B_k - 0.63)^2$ where $B_k$ is the proportion of blacks by town\n",
    "    \n",
    "    13. LSTAT    lower status of the population\n",
    "    \n",
    "    14. MEDV     Median value of owner-occupied homes in \\$1000's\n",
    "\n",
    "8. Missing Attribute Values:  None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Para importar el dataset, se utiliza keras, aunque también puede ser extraído de GitHub\n",
    "from keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 2us/step\n"
     ]
    }
   ],
   "source": [
    "# Según la documentación oficial https://keras.io/datasets/#boston-housing-price-regression-dataset\n",
    "# se devuelven dos tuplas con los datos ya divididos\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de casas y variables por cada una: (404, 13)\n"
     ]
    }
   ],
   "source": [
    "# Los objetos devueltos son arreglos de numpy\n",
    "print('Número de casas y variables por cada una: {0}'.format(x_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'], dtype='<U7')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pero los nombres de las variables no están en el conjunto, por tanto se añaden\n",
    "nombres = [\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"MEDV\"]\n",
    "# Sólo se reemplazan dobles comillas por comillas simples para consistencia de Python\n",
    "column_names = np.array([i.replace('\\\"', '\\'') for i in nombres])\n",
    "column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Números de NIST, modificados (MNIST dataset)\n",
    "\n",
    "Este conjunto de datos contiene imágenes de números escritos por personas para el NIST, el instituto de metrología de los Estados Unidos. Son 70,000 imágenes en blanco y negro de tamaño $28 \\times 28$ pixeles, y que están disponibles de forma gratuita en [este sitio](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "Este dataset es ideal para las tareas de clasificación y reconocimiento de patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sin embargo, con keras se pueden importar\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Nuevamente, preparar dos tuplas para guardar los datos de prueba y de entrenamiento\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Los datos de entrenamiento son 60000 imágenes\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número real de esta imagen: 5\n"
     ]
    }
   ],
   "source": [
    "# Se puede ver la primera imagen con matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_train[0].reshape((28, 28)))\n",
    "# Esta puede no ser la primera imagen del conjunto original\n",
    "plt.show()\n",
    "# Imprimir el valor real de esta imagen\n",
    "print('Número real de esta imagen: {0}'.format(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este conjunto de datos contiene mensajes categorizados como _ham_ para mensajes que provienen tanto de personas como de interacciones reales, o _spam_ para mostrar que son mensajes falsos. Es un dataset muy interesante porque permite el análisis de texto en una de las tantas ramas de la inteligencia artificial que se llama _procesamiento del lenguaje natural._\n",
    "\n",
    "Para más información del dataset, visitar la [página](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection#) de la University of California at Irvine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para descargar los datos, se utilizará la página de la UCI\n",
    "# https://archive.ics.uci.edu/ml/machine-learning-databases/00228/\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar el archivo y descomprimir\n",
    "file = requests.get('https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip')\n",
    "# Se debe convertir primero a tipo bytes para poder leer el contenido de un zipfile\n",
    "file = ZipFile(io.BytesIO(file.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodificar el texto\n",
    "text_data = file.read('SMSSpamCollection').decode()\n",
    "# Y partirlo en líneas por separado\n",
    "text_data = text_data.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases a las que pertenecen los textos: ['ham' 'ham' 'spam' ... 'ham' 'ham' 'ham']\n",
      "\n",
      "Textos de SMS\n",
      "[['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...']\n",
      " ['Ok lar... Joking wif u oni...']\n",
      " [\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"]\n",
      " ...\n",
      " ['Pity, * was in mood for that. So...any other suggestions?']\n",
      " [\"The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free\"]\n",
      " ['Rofl. Its true to its name']]\n"
     ]
    }
   ],
   "source": [
    "# Al igual que antes, quitar los tabuladores\n",
    "text_data = np.array([i.split('\\t') for i in text_data])\n",
    "# Si se hace array indexing con numpy se tienen las columnas por separado\n",
    "text_target = text_data[:, 0]\n",
    "print('Clases a las que pertenecen los textos: {0}\\n'.format(text_target))\n",
    "# Se debe reajustar el tamaño para separar cada línea como un arreglo\n",
    "text_data = text_data[:, 1].reshape((len(text_data), -1))\n",
    "print('Textos de SMS')\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valoraciones de las películas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este dataset contiene datos para realizar _análisis de opiniones_ (también conocido en inglés como [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis)).\n",
    "\n",
    "En particular el dataset escogido cuenta con opiniones y valoraciones en texto (en inglés) de películas, espcíficamente 5331 valoraciones _positivas_ y 5331 _negativas_. Para más información del dataset, visitar la [página oficial](https://www.cs.cornell.edu/people/pabo/movie-review-data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como el conjunto de datos viene comprimido en formato tar.gz\n",
    "# se debe importar la librería pertinente\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos la URL\n",
    "url = 'https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "# Y realizamos el request\n",
    "data = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el fichero en formato de bytes como antes\n",
    "stream_data = io.BytesIO(data.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dado que iremos guardando la información línea por línea\n",
    "# creamos una variable temporal para guardar toda la información\n",
    "tmp = io.BytesIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora se debe leer cada línea del archivo y guardarla en la variable temporal\n",
    "for line in stream_data:\n",
    "    tmp.write(line)\n",
    "# Y cerrar el achivo de streaming original\n",
    "stream_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se debe regresar al principio del archivo\n",
    "tmp.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando la librería estándar de Python, se descomprime y se lee el fichero\n",
    "tar_file = tarfile.open(fileobj=tmp, mode='r:gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se extraen los dos tipos de archivos dentro del directorio de datos\n",
    "pos = tar_file.extractfile('rt-polaritydata/rt-polarity.pos')\n",
    "neg = tar_file.extractfile('rt-polaritydata/rt-polarity.neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para las valoraciones positivas se crea una lista para guardar todos los datos\n",
    "pos_data = np.array([])\n",
    "for line in pos:\n",
    "    # Dado que los archivos de información vienen en codificaciones especiales, se deben decodificar\n",
    "    # y luego volver a codificar en datos que Python pueda entender, ignorando errores en el camino\n",
    "    pos_data = np.append(pos_data, line.decode('ISO-8859-1').encode('ascii', errors='ignore').decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5331,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se puede corroborar que se tiene el número correcto de datos\n",
    "pos_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \\n'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y para comprobar, se puede leer la primera entrada\n",
    "pos_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para las valoraciones negativas se realiza lo mismo\n",
    "neg_data = np.array([])\n",
    "for line in neg:\n",
    "    neg_data = np.append(neg_data, line.decode('ISO-8859-1').encode('ascii', errors='ignore').decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5331,)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se puede corroborar que se tiene el número correcto de datos,\n",
    "# deben ser los mismos que para las valoraciones positivas\n",
    "neg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simplistic , silly and tedious . \\n'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y para comprobar, se puede leer la primera entrada\n",
    "neg_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este conjunto de datos contiene 60000 imágenes de 32x32 pixeles, a color, categorizadas en **10** categorías diferentes. En particular, este conjunto de datos fue etiquetado _a mano_ por gente de la _University of Toronto_, por lo que se puede asegurar que los resultados a esperar deben de ser los correctos.\n",
    "\n",
    "El _objetivo_ de este conjunto de imágenes es la clasificación de cada imagen en su respectiva categoría. Los datos vienen en _lotes_ o _batches_ dado que son muchas imágenes. El fichero completo _pesa_ alrededor de 180 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este es el url del fichero, por el momento no se descargará\n",
    "url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shakespeare en texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las ramas más importante de la inteligencia _artificial_ es el Procesamiento de Lenguaje Natural (**NLP** por sus siglas en inglés). Esta rama se encarga de la parte lingüística de la inteligencia computacional donde se entrenan redes neuronales para realizar estudios detallados de textos, su estructura sintáctica y gramatical, y poder crear modelos a partir de este tipo de análisis.\n",
    "\n",
    "En particular, este conjunto de datos pretende extraer todas las obras de William Shakespeare y con ellas entrenar una red neuronal que tenga la habilidad de crear textos con la misma estructura que un texto de Shakespear, i.e. crear una inteligencia artificial que haga las veces de William Shakespeare. Este conjunto de datos está gratis en el sitio web de [Project Gutenberg.](https://www.gutenberg.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta es la dirección completa del libro que contiene todas las obras de W. Shakespeare\n",
    "url = 'http://www.gutenberg.org/files/100/100-0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se hace la solicitud\n",
    "req = requests.get(url)\n",
    "# Y se guarda el archivo\n",
    "shak_file = req.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para tener el texto como tal, se debe decodificar\n",
    "shak_text = shak_file.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff\\r\\nProject Gutenberg’s The Complete Works of William Shakespeare, by William\\r\\nShakespeare\\r\\n\\r\\nThis eBook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever.  You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg License included with this eBook or online at\\r\\nwww.gutenberg.org.  If you are not located in the United States, you’ll\\r\\nhave to check the laws of the country where you are located before using\\r\\nthis ebook.\\r\\n\\r\\nSee at the end of this file: * CONTENT NOTE (added in 2017) *\\r\\n\\r\\n\\r\\nTitle: The Complete Works of William Shakespeare\\r\\n\\r\\nAuthor: William Shakespeare\\r\\n\\r\\nRelease Date: January 1994 [EBook #100]\\r\\nLast Updated: March 9, 2019\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCharacter set encoding: UTF-8\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nThe Complete Works of William Shakespeare\\r\\n\\r\\n\\r\\n\\r\\nby William Shakespeare\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n '"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y se exploran los primeros caracteres\n",
    "shak_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como se puede observar, el texto contiene un encabezado que no aporta a los datos\n",
    "# por lo que es buena medida quitar este texto\n",
    "shak_text = shak_text[3009:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Al final del texto también contiene la licencia para redistribuir el texto, pero no\n",
    "# se debe mantener para el análisis de texto, por lo que se quita\n",
    "shak_text = shak_text[:-21550]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _NOTA_: La _limpieza_ y reestructuración del texto de Shakespeare fue a mano, según los cambios que realice Project Gutenberg sobre este texto, esto se tendría que realiza nuevamente a mano según el usuario, si lo quisera hacer de nuevo desde cero. Sin embargo, en esta libreta los cambios se quedan guardados y no haría falta hacerlo de nuevo manualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traducción entre idiomas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como parte del NLP en la inteligencia artificial se puede realizar una _traducción en linea_ donde se cree un modelo que sirva de traductor entre dos idiomas. Este caso se puede observar como un caso particular de clasificación, pero es un tanto más complicado que eso dado que se debe analizar muy bien el texto y su estructura, palabra por palabra.\n",
    "\n",
    "Para este conjunto de datos se hará uso del trabajo ya realizado por el proyecto [Tatoeba](https://tatoeba.org/eng) y utilizar un proyecto hermano [ManyThings](http://www.manythings.org/) para obtener un archivo con oraciones ya traducidas entre dos idiomas. En particular se realizará ahora para el inglés-español, posteriormente se podrá utilizar algún otro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el URL para inglés-español\n",
    "url = 'http://www.manythings.org/anki/spa-eng.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer la solicitud al servidor\n",
    "req = requests.get(url)\n",
    "# Y dado que es un fichero comprimido zip, se debe manipular y extraer\n",
    "z = ZipFile(io.BytesIO(req.content))\n",
    "# Posteriormente, leer todo el archivo descomprimido\n",
    "file = z.read('spa.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se debe decodificar el archivo\n",
    "data = file.decode()\n",
    "# Y codificar en el estándar utf-8, ignorando errores por ahora\n",
    "data = data.encode('utf-8', errors = 'ignore')\n",
    "# Para su mejor manipulación, se separa en lineas\n",
    "data = data.decode().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.\\tVe.',\n",
       " 'Go.\\tVete.',\n",
       " 'Go.\\tVaya.',\n",
       " 'Go.\\tVáyase.',\n",
       " 'Hi.\\tHola.',\n",
       " 'Run!\\t¡Corre!',\n",
       " 'Run.\\tCorred.',\n",
       " 'Who?\\t¿Quién?',\n",
       " 'Wow!\\t¡Órale!',\n",
       " 'Fire!\\t¡Fuego!']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se pueden explorar las primeras 10 lineas del archivo\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Go.', 'Ve.'],\n",
       "       ['Go.', 'Vete.'],\n",
       "       ['Go.', 'Vaya.'],\n",
       "       ['Go.', 'Váyase.'],\n",
       "       ['Hi.', 'Hola.'],\n",
       "       ['Run!', '¡Corre!'],\n",
       "       ['Run.', 'Corred.'],\n",
       "       ['Who?', '¿Quién?'],\n",
       "       ['Wow!', '¡Órale!'],\n",
       "       ['Fire!', '¡Fuego!']], dtype='<U332')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora se debe quitar los tabuladores, y se convierte a un arreglo de numpy\n",
    "# para mejor manejo de los índices\n",
    "data = np.array([x.split('\\t') for x in data if len(x)>=1])\n",
    "# Y se exploran las primeras 10 lineas del archivo\n",
    "data[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por último, se separan las columnas entre español e inglés\n",
    "eng_data = data[:, 0]\n",
    "spa_data = data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go.' 'Go.' 'Go.' 'Go.' 'Hi.' 'Run!' 'Run.' 'Who?' 'Wow!' 'Fire!']\n",
      "['Ve.' 'Vete.' 'Vaya.' 'Váyase.' 'Hola.' '¡Corre!' 'Corred.' '¿Quién?'\n",
      " '¡Órale!' '¡Fuego!']\n"
     ]
    }
   ],
   "source": [
    "# Se pueden explorar los primeros datos\n",
    "print(eng_data[:10])\n",
    "print(spa_data[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
