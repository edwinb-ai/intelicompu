{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximización de los valores de expectación / EM\n",
    "\n",
    "Es un proceso iterativo utilizado para encontrar el mejor valor de expectación. Se parte de una suposición inicial de los centroides de las clases y mediane un proceso iterativo que busca maximizar la probabilidad de los puntos a pertencer a una u otra clase. \n",
    "\n",
    "El algoritmo EM se utiliza para encontrar maximos locales de probabilidad cuando los modelos contienen variables inferidas o [variables latentes](https://es.wikipedia.org/wiki/Variable_latente), así como datos y parámetos faltantes. El modelo parte de asumir que a cada punto de los datos conocidos le corresponde uno faltante. En partícular se busca maximizar la [probabilidad a posteriori](https://es.wikipedia.org/wiki/Probabilidad_a_posteriori) de que un punto pertenezca a una clase.\n",
    "La siguiente animación tomada de [Wikipedia](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#/media/File:EM_Clustering_of_Old_Faithful_data.gif) muestra la convergencia del modelo.\n",
    "\n",
    "Volviendo al ejemplo sobre las etiquetas de genero de la sección anterior, esta información son los datos faltantes que se mencionan aquí, los cuales corresponden con las etiquetas o clases del archivo de datos. \n",
    "\n",
    "EM asigna los puntos que deseamos clasificar a una clase en base a una probabilidad.\n",
    "\n",
    "El algoritmo EM funciona iterativamente aplicando dos pasos:\n",
    "\n",
    "1. **Expectación** (E): Assignar los puntos al cluster más cercano.\n",
    "* **Maximización** (M): Calcular la media de las nuevas distribuciones de cada clase.\n",
    "\n",
    "En el paso E se actualizan los valores de expectación mediane la función de probabilidad condicional de que los puntos pertenezcan a una clase:\n",
    "\n",
    "$$Q(\\theta|\\theta^{(t)}) = E_{Z|X,\\theta^{(t)}}[logL(\\theta,X,Z]$$\n",
    "\n",
    "En donde $X$ son los datos de observación, $Z$ son los valores falantes y $\\theta$ un vector de los parámetros conocidos.\n",
    "\n",
    "Y posteriormente se busca maximizar la expresión anterior con respecto a los parámetros $\\theta$.\n",
    "\n",
    "Si se aprender más sobre el algoritmo, fundamentación y derivación matemática, se recomiendan los siguientes artículos:\n",
    "\n",
    "[Frank Dellaert, The Expectation Maximization Algorithm, 2002](https://www.cc.gatech.edu/~dellaert/em-paper.pdf).\n",
    "\n",
    "[Sean Borman, The Expectation Maximization Algorithm A short tutorial](https://www.seanborman.com/publications/EM_algorithm.pdf)\n",
    "\n",
    "[Chuong B Do & Serafim Batzoglou, What is the expectation maximization algorithm?, 2008](https://www.nature.com/articles/nbt1406)\n",
    "\n",
    "## EM en Python\n",
    "\n",
    "Para ejemplificar el uso de EM en Python, se utilizará el famoso ejemplo del _experimento del volado con una moneda_. Para ello se utiliza el siguiente artículo de _Nature_ publicado en _Computational Biology_: [Chuong B Do & Serafim Batzoglou](http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf)\n",
    "\n",
    "Primeramente imagina que tienes dos monedas, $A$ y $B$, de sesgos desconocidos, $\\theta_A$ y $\\theta_B$ respectivamente. Las proabilidades son entonces $\\theta_A$ la probabilidad de obtener cara y $1-\\theta_A$ de obtener cruz, para la moneda $A$, y similar para la moneda $B$. Nuestro objetivo es entonces conocer el sesgo $\\theta = (\\theta_A,\\theta_B)$. Para ello se repetirán los siguientes pasos 5 veces:\n",
    "\n",
    "1. Escoger una moneda al azar (con igual probabilidad).\n",
    "2. Realizar 10 lanzamientos con la moneda elegida.\n",
    "\n",
    "Durante el experimento guardamos el número de caras del $i$-ésimo lanzamiento, $x_i$ y una etiqueta que identifique a dicha moneda, $z_i$. Buscamos entonces conocer el máximo valor de expectación para cada moneda.\n",
    "\n",
    "Podemos estimar $\\theta_A$ y $\\theta_B$ de la siguiente manera:\n",
    "\n",
    "$$\\hat{\\theta}_A = \\frac{n_A}{N_A}$$\n",
    "\n",
    "y\n",
    "\n",
    "$$\\hat{\\theta}_B = \\frac{n_B}{N_B}$$\n",
    "\n",
    "en donde $n_A$ es el número de caras obtenidas utilizando la moneada A y $N_A$ es el número total de lanzamientos; similar para la moneda B.\n",
    "\n",
    "Ahora, $log P(x,z;\\theta)$ es la probabilidad de obtener cualquiera de los vectores en partícular, de la cuenta del número de caras $x$ de la moneza $z$, de manera que las expresiones anteriores resolverían el problema de manera muy simple.\n",
    "\n",
    "Ésto se parece, en nuestro ejemplo de introducción, al caso en el cual se tiene un archivo de datos completo; se conoce a qué moneda le pertenece cada conjunto de datos de los lanzamientos, así como se conocían las etiquetas del genero en nuestro ejemplo.\n",
    "\n",
    "Ahora imaginate que sólo conocemos la información del número de caras obtenidas, pero no su identidad de a qué moneda le pertenece cada lanzamiento de dicha información. Entonces la información faltante, $Z$, son si los lanzamientos pertenecen a  la moneda A o B. \n",
    "\n",
    "El objetivo es determinar, para cada uno de los cinco experimentos, si los lanzamientos se hicieron utilizando la moneda A o la moneda B.\n",
    "\n",
    "!TERMINAR EL EJEMPLO¡\n",
    "\n",
    "# Mezclas Gausianas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
